{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to NLP\n",
    "\n",
    "NLP pipelines in general consist of three stages:\n",
    "\n",
    "* Text Processing\n",
    "* Feature Extraction\n",
    "* Modeling\n",
    "\n",
    "**text Processing Steps:**\n",
    "\n",
    "* Tokenization — convert sentences to words\n",
    "* Removing unnecessary punctuation, tags\n",
    "* Change Capitalization\n",
    "* Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic\n",
    "* Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
    "* Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "**Bag-of-words**\n",
    "\n",
    "treats each document as a unordered collection of words. Turns each document into a vector of numbers, basically how many times a word has been occured in a document.\n",
    "\n",
    "* collect unique words in corpus to form a vocabulary\n",
    "* form Document-Term Matrix\n",
    "* compare 2 documents using \"dot product\" of 2 vectors\n",
    "$$\n",
    "ab = \\sum a_0 b_0 + a_1 b_1 + ... + a_n b_n \n",
    "$$\n",
    "* greater the dot product of 2 vectors, the more similar they are\n",
    "* dot product is flawed, it only captures the portions of overlaps, so very diffrent vectors can have the same product of very similar vectors. \n",
    "* better measure is **cosine similarity**\n",
    "$$ \n",
    "cos(\\Theta) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}\n",
    "$$\n",
    "where $||a|| = \\sqrt{a_1^2 + a_2^2 + ... + a_n^2}$ represents their magnitudes or Euclidian norms.\n",
    "\n",
    "if imagine vectors in n-dimensional space, cosine similarity means the cosine of angle between 2 vectors:\n",
    "* Identical vectors have cosine of 1\n",
    "* Orthogonal vectors have cosine of 0\n",
    "* Opposite vectors have cosine of -1\n",
    "\n",
    "values of cosine are always between **[-1, +1]**\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "One limiation of Bag of words: Treats every word as being equally important.\n",
    "\n",
    "We can approach this by collecting each word's frequency and then dividing the term frequencies by the document frequency of that term to get a relative measure.\n",
    "\n",
    "$$\n",
    "tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D),\n",
    "$$\n",
    "\n",
    "\n",
    "The total number of documents in the collection, divided by the number of documents where t is present. tf-idf assigns weight to words that signify their relevance in the document.\n",
    "\n",
    "**One-Hot Encoding**\n",
    "\n",
    "In the context of Language Processing One-hot encoding means:\n",
    "\n",
    "Treat each word like a class, assign it a vector. If the word is present this variable is one and zero otherwise.\n",
    "\n",
    "**Word Embedding*\n",
    "\n",
    "* One-Hot Encoding breaks down when we have a large vocabulary to deal with, because the size of word representation grows with the number of words in the dictionary.\n",
    "* Word Embedding provides fixed length numeric vector to represnt a word instead of its characters.\n",
    "* every word in your dictionary has a unique vector associated with it.\n",
    "\n",
    "**Embedding technique 1: Word2Vec**\n",
    "\n",
    "Word2Vec is perhaps one of the most popular examples of word embeddings used in practice. \n",
    "\n",
    "* SkipGram: \n",
    "- summary: pick a window size that count as your neighbord radius, any word inside this window will be the context.\n",
    "- Pick any word from a sentence,\n",
    "- convert it into a one-hot encoded vector and feed it into a neural network (or some ohter probabilistic model).\n",
    "- Train model to predict context words as best as it can.\n",
    "- Take an intermediate representation like a hidden layer in a neural network.\n",
    "- Outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "**Embedding technique 2: GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
