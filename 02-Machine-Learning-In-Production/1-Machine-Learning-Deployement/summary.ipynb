{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow\n",
    "\n",
    "![ML work flow](../../images/workflow.png)\n",
    "\n",
    "### Cloud Computing\n",
    "\n",
    "Cloud computing can simply be thought of as transforming an Information Technology (IT) product into a service.\n",
    "This ability to easily meet unstable, fluctuating customer demand illustrates many of the benefits of cloud computing.\n",
    "\n",
    "#### Benefits\n",
    "1. Reduced Investments and Proportional Costs (providing cost reduction)\n",
    "2. Increased Scalability (providing simplified capacity planning)\n",
    "3. Increased Availability and Reliability (providing organizational agility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment to production** can simply be thought of as a method that integrates a machine learning model into an existing production environment so that the model can be used to make decisions or predictions based upon data input into the model.\n",
    "\n",
    "### Paths to Deployment\n",
    "1. Python model is recoded into the programming language of the production environment.\n",
    "\n",
    "The first method which involves recoding the Python model into the language of the production environment, often Java or C++. This method is rarely used anymore because it takes time to recode, test, and validate the model that provides the same predictions as the original.\n",
    "\n",
    "2. Model is coded in Predictive Model Markup Language (PMML) or Portable Format Analytics (PFA).\n",
    "\n",
    "The second method is to code the model in Predictive Model Markup Language (PMML) or Portable Format for Analytics (PFA), which are two complementary standards that simplify moving predictive models to deployment into a production environment. The Data Mining Group developed both PMML and PFA to provide vendor-neutral executable model specifications for certain predictive models used by data mining and machine learning. Certain analytic software allows for the direct import of PMML including but not limited to IBM SPSS, R, SAS Base & Enterprise Miner, Apache Spark, Teradata Warehouse Miner, and TIBCO Spotfire.\n",
    "\n",
    "3. Python model is converted into a format that can be used in the production environment.\n",
    "\n",
    "The third method is to build a Python model and use libraries and methods that convert the model into code that can be used in the production environment. Specifically most popular machine learning software frameworks, like PyTorch, TensorFlow, SciKit-Learn, have methods that will convert Python models into intermediate standard format, like ONNX ([Open Neural Network Exchange format](https://onnx.ai/)). This intermediate standard format then can be converted into the software native to the production environment.\n",
    "\n",
    "* This is the easiest and fastest way to move a Python model from modeling directly to deployment.\n",
    "* Moving forward this is typically the way models are moved into the production environment.\n",
    "* Technologies like containers, endpoints, and APIs (Application Programming Interfaces) also help ease the work required for deploying a model into the production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workflow and Deployment](../../images/devOps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Environment and the Endpoint\n",
    "\n",
    "the endpoint was defined as the interface to the model. This interface (endpoint) facilitates an ease of communication between the model and the application. Specifically, this interface (endpoint)\n",
    "\n",
    "* Allows the application to send user data to the model and\n",
    "* Receives predictions back from the model based upon that user data.\n",
    "\n",
    "Communication between the application and the model is done through the endpoint (interface), where the endpoint is an Application Programming Interface (API).\n",
    "\n",
    "* An easy way to think of an API, is as a set of rules that enable programs, here the application and the model, to communicate with each other.\n",
    "\n",
    "* In this case, our API uses a REpresentational State Transfer, REST, architecture that provides a framework for the set of rules and constraints that must be adhered to for communication between programs.\n",
    "\n",
    "* This REST API is one that uses HTTP requests and responses to enable communication between the application and the model through the endpoint (interface).\n",
    "\n",
    "* Noting that both the HTTP request and HTTP response are communications sent between the application and model.\n",
    "\n",
    "#### The HTTP request that’s sent from your application to your model is composed of four parts:\n",
    "\n",
    "1. Endpoint: This endpoint will be in the form of a URL, Uniform Resource Locator, which is commonly known as a web address.\n",
    "2. HTTP Method: Below you will find four of the HTTP methods, but for purposes of deployment our application will use the POST method only.\n",
    "3. HTTP Headers: The headers will contain additional information, like the format of the data within the message, that’s passed to the receiving program.\n",
    "4. Message (Data or Body): The final part is the message (data or body); for deployment will contain the user’s data which is input into the model.\n",
    "\n",
    "![HTTP request](../../images/httprequest.png)\n",
    "\n",
    "#### The HTTP response sent from your model to your application is composed of three parts:\n",
    "\n",
    "1. HTTP Status Code: If the model successfully received and processed the user’s data that was sent in the message, the status code should start with a 2, like 200.\n",
    "2. HTTP Headers: The headers will contain additional information, like the format of the data within the message, that’s passed to the receiving program.\n",
    "3. Message (Data or Body): What’s returned as the data within the message is the prediction that’s provided by the model.\n",
    "\n",
    "**Summary:** This prediction is then presented to the application user through the application. The endpoint is the interface that enables communication between the application and the model using a REST API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containers\n",
    "\n",
    "![container](../../images/containers.png)\n",
    "\n",
    "**The model** is simply the Python model that's created, trained, and evaluated in the Modeling component of the machine learning workflow.\n",
    "**The application** is simply a web or software application that enables the application users to use the model to retrieve predictions.\n",
    "\n",
    "Both the model and the application require a computing environment so that they can be run and available for use. One way to create and maintain these computing environments is through the **use of containers**.\n",
    "\n",
    "**A container** can be thought of as a standardized collection/bundle of software that is to be used for the specific purpose of running an application. container technology is used to create the model and application computational environments associated with deployment in machine learning. A common container software is **Docker**.\n",
    "\n",
    "**Docker containers:**\n",
    "* Can contain all types of different software.\n",
    "* The structure of a Docker container enables the container to be created, saved, used, and deleted through a set of common tools.\n",
    "* The common tool set works with any container regardless of the software the container contains.\n",
    "\n",
    "![docker](../../images/docker.png)\n",
    "\n",
    "#### Container Structure\n",
    "1. The underlying computational infrastructure which can be: a cloud provider’s data center, an on-premise data center, or even someone’s local computer.\n",
    "2. Next, you have an operating system running on this computational infrastructure, this could be the operating system on your local computer.\n",
    "3. Next, there’s the container engine, this could be Docker software running on your local computer. The container engine software enables one to create, save, use, and delete containers; for our example, it could be Docker running on a local computer.\n",
    "4. The final two layers make up the composition of the containers.The first layer of the container is the libraries and binaries required to launch, run, and maintain the next layer, the application layer.\n",
    "\n",
    "**The image above shows three containers running three different applications.**\n",
    "\n",
    "#### Container Advantages\n",
    "\n",
    "1. Isolates the application, which increases security.\n",
    "2. Requires only software needed to run the application, which uses computational resources more efficiently and allows for faster application deployment.\n",
    "3. Makes application creation, replication, deletion, and maintenance easier and the same across all applications that are deployed using containers.\n",
    "4. Provides a more simple and secure way to replicate, save, and share containers.\n",
    "\n",
    "**a container script file is used to create a container.**\n",
    "* This text script file can easily be shared with others and provides a simple method to replicate a particular container.\n",
    "* This container script is simply the instructions (algorithm) that is used to create a container; for Docker these container scripts are referred to as dockerfiles.\n",
    "\n",
    "This is shown with the image below, where the container engine uses a container script to create a container for an application to run within. These container script files can be stored in repositories, which provide a simple means to share and replicate containers. For Docker, the [Docker Hub](https://hub.docker.com/search?q=&type=image) is the official repository for storing and sharing dockerfiles. Here's an example of a [dockerfile](https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile) that creates a docker container with Python 3.6 and PyTorch installed.\n",
    "\n",
    "\n",
    "![docker](../../images/docker2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **hyperparameter** is not directly learned through the estimators; therefore, their value must be set by the model developer. This means that hyperparameter tuning for optimization is an important part of model training.\n",
    "\n",
    "* Often cloud platform machine learning services provide methods that allow for automatic hyperparameter tuning for use with model training.\n",
    "\n",
    "* If the machine learning platform fails to offer an automatic hyperparameter option, one option is to use methods from scikit-learn Python library for hyperparameter tuning. Scikit-learn is a free machine learning Python library that includes methods that help with [hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of Deployment\n",
    "\n",
    "![deployement](../../images/deployement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Web Services (AWS)\n",
    "\n",
    "Amazon Web Services (AWS) SageMaker is Amazon's cloud service that allows you to build, train, and deploy machine learning models. Some advantages to using Amazon's SageMaker service are the following:\n",
    "\n",
    "* **Flexibility in Machine Learning Software:** SageMaker has the flexibility to enable the use of any programming language or software framework for building, training, and deploying machine learning models in AWS. For the details see the three methods of modeling within SageMaker below.\n",
    "\n",
    "    * **Built-in Algorithms** - There are at least fifteen built-in algorithms that are easily used within SageMaker. Specifically, built-in algorithms for discrete classification or quantitative analysis using linear learner or XGBoost, item recommendations using factorization machine, grouping based upon attributes using K-Means, an algorithm for image classification, and many other algorithms.\n",
    "    * **Custom Algorithms** - There are different programming languages and software frameworks that can be used to develop custom algorithms which include: PyTorch, TensorFlow, Apache MXNet, Apache Spark, and Chainer.\n",
    "    * **Your Own Algorithms** - Regardless of the programming language or software framework, you can use your own algorithm when it isn't included within the built-in or custom algorithms above.\n",
    "    \n",
    " \n",
    "* **Ability to Explore and Process Data within SageMaker:** SageMaker enables the use of Jupyter Notebooks to explore and process data, along with creation, training, validation, testing, and deployment of machine learning models. This notebook interface makes data exploration and documentation easier.\n",
    "\n",
    "\n",
    "* **Flexibility in Modeling and Deployment:** SageMaker provides a number of features and automated tools that make modeling and deployment easier. For the details on these features within SageMaker see below.\n",
    "\n",
    "    * **Automatic Model Tuning:** SageMaker provides a feature that allows hyperparameter tuning to find the best version of the model for built-in and custom algorithms. For built-in algorithms SageMaker also provides evaluation metrics to evaluate the performance of your models.\n",
    "    * Monitoring Models: SageMaker provides features that allow you to monitor your deployed models. Additionally with model deployment, one can choose how much traffic to route to each deployed model (model variant). More information on routing traffic to model variants can be found [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProductionVariant.html) and [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html).\n",
    "    \n",
    "    \n",
    "* **Type of Predictions:** SageMaker by default allows for On-demand type of predictions where each prediction request can contain one to many requests. SageMaker also allows for Batch predictions, and request data size limits are based upon S3 object size limits.\n",
    "\n",
    "#### Main diffrence between Google Cloud Platform (GCP) and Amazon Web Services (AWS) SageMaker:\n",
    "\n",
    "**Prediction Costs:** The primary difference between the two is how they handle predictions. With SageMaker predictions, you must leave resources running to provide predictions. This enables less latency in providing predictions at the cost of paying for running idle services, if there are no (or few) prediction requests made while services are running. With ML Engine predictions, one has the option to not leave resources running which reduces cost associated with infrequent or periodic requests. Using this has more latency associated with predictions because the resources are in a offline state until they receive a prediction request. The increased latency is associated to bringing resources back online, but one only pays for the time the resources are in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
