{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos\n",
    "[Intro](https://www.youtube.com/watch?v=ohVX3RUTghg&feature=emb_logo&ab_channel=Udacity)\n",
    "\n",
    "[Hyperparameter Tunning SageMaker](https://www.youtube.com/watch?v=nah8kxqp55U&feature=emb_logo&ab_channel=Udacity)\n",
    "\n",
    "[Set up Hyperparameter Tuner](https://www.youtube.com/watch?v=lsYRtKivrGc&feature=emb_logo&ab_channel=Udacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "In many machine learning models there are some parameters that need to be specified by the model creator and which can't be determined directly from the data itself. Generally the approach to finding the best parameters is to train a bunch of models with different parameters and then choose the model that works best.\n",
    "\n",
    "SageMaker provides an automated way of doing this. In fact, SageMaker also does this in an intelligent way using Bayesian optimization. What we will do is specify ranges for our hyperparameters. Then, SageMaker will explore different choices within those ranges, increasing the performance of our model over time.\n",
    "\n",
    "In addition to learning how to use hyperparameter tuning, we will look at Amazon's CloudWatch service. For our purposes, CloudWatch provides a user interface through which we can examine various logs generated during training. This can be especially useful when diagnosing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, tuning a model means training a bunch of models, each with different hyperparameters, and then choosing the best performing model. Of course, we still need to describe two different aspects of hyperparameter tuning:\n",
    "\n",
    "1) What is a bunch of models? In other words, how many different models should we train?\n",
    "\n",
    "2) Which model is the best model? In other words, what sort of metric should we use in order to distinguish how well one model performs relative to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, the way to think about hyperparameter tuning inside of SageMaker is that we start with a base collection of hyperparameters which describe a default model. We then give some additional set of hyperparameters ranges. These ranges tell SageMaker which hyperparameters can be varied, with the goal being to improve the default model.\n",
    "\n",
    "We then describe how to compare models, which in our instance is just by way of specifying a metric. Then we describe how many total models we want SageMaker to train.\n",
    "\n",
    "**Note:** In addition to creating a tuned model in this notebook, we also saw how the **attach method** can be used **to create an Estimator object which is attached to an already completed training job**. This method is useful in other situations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
