{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Ground Truth\n",
    "\n",
    "- use a combinations of **human labelers** and **active learning model** to efficiently label data.\n",
    "- reduce the time and cost to label datasets by 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=Cf_LSDCEBzk)\n",
    "\n",
    "You select k, a predetermined number of clusters that you want to form. Then k points (centroids for k clusters) are selected at random locations in feature space. For each point in your training dataset:\n",
    "\n",
    "1. You find the centroid that the point is closest to\n",
    "2. And assign that point to that cluster\n",
    "3. Then, for each cluster centroid, you move that point such that it is in the center of all the points that are were assigned to that cluster in step 2.\n",
    "4. Repeat steps 2 and 3 until you’ve either reached convergence and points no longer change cluster membership or until some specified number of iterations have been reached.\n",
    "\n",
    "\n",
    "### Choosing a \"Good\" K\n",
    "\n",
    "One method for choosing a \"good\" k, is to choose based on empirical data.\n",
    "\n",
    "- A bad k would be one so high that only one or two very close data points are near it, and\n",
    "- Another bad k would be one so low that data points are really far away from the centers.\n",
    "\n",
    "You want to select a k such that data points in a single cluster are close together but that there are enough clusters to effectively separate the data. You can approximate this separation by measuring how close your data points are to each cluster center; the average centroid distance between cluster points and a centroid. After trying several values for k, the centroid distance typically reaches some \"elbow\"; it stops decreasing at a sharp rate and this indicates a good value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dimensionality\n",
    "One thing to note is that it’s often easiest to form clusters when you have **low-dimensional data**. For example, it can be difficult, and often noisy, to get good clusters from data that has over 100 features. In high-dimensional cases, there is often a dimensionality reduction step (like PCA) that takes place before data is analyzed by a clustering algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "\n",
    "To make sure the feature measurements are consistent and comparable, you’ll scale all of the numerical features into a range between 0 and 1. This is a pretty typical normalization step.\n",
    "\n",
    "1. Rescaling (min-max normalization)\n",
    "\n",
    "data will be in [-1, +1] range\n",
    "\n",
    "$\n",
    "x'={\\frac  {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}\n",
    "$\n",
    "\n",
    "2. Mean normalization \n",
    "\n",
    "centers data around mean point [-3, +3]\n",
    "\n",
    "$\n",
    "{\\displaystyle x'={\\frac {x-{\\text{average}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}}\n",
    "$\n",
    "3. Standardization (Z-score Normalization)\n",
    "\n",
    "$\n",
    "x' = \\frac{x - \\bar{x}}{\\sigma}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=uyl44T12yU8)\n",
    "\n",
    "Principal Component Analysis (PCA) attempts to reduce the number of features within a dataset while retaining the “principal components”, which are defined as weighted combinations of existing features that:\n",
    "\n",
    "1. Are uncorrelated with one another, so you can treat them as independent features, and\n",
    "2. Account for the largest possible variability in the data!\n",
    "\n",
    "So, depending on how many components we want to produce, the first one will be responsible for the largest variability on our data and the second component for the second-most variability, and so on. Which is exactly what we want to have for clustering purposes!\n",
    "\n",
    "**PCA is commonly used when you have data with many many features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker example repository\n",
    "https://github.com/aws/amazon-sagemaker-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Model\n",
    "To define a custom model, you need to have the model itself and the following two scripts:\n",
    "\n",
    "1. A training script that defines how the model will accept input data, and train. This script should also save the trained model parameters.\n",
    "2. A predict script that defines how a trained model produces an output and in what format.\n",
    "\n",
    "### PyTorch\n",
    "In PyTorch, you have the option of defining a neural network of your own design. These models do not come with any built-in predict scripts and so you have to write one yourself.\n",
    "\n",
    "### SKLearn\n",
    "The scikit-learn library, on the other hand, has many pre-defined models that come with train and predict functions attached!\n",
    "\n",
    "You can define custom SKLearn models in a very similar way that you do PyTorch models only you typically only have to define the training script. You can use the default predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
